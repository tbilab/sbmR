---
title: "Agglomerative Merging"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{agglomerative_merging}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r load-packages, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  warning = FALSE
)

library(sbmr)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
```

One technique of finding the optimal number of clusters/ best partitioning is using the efficient agglomerative merging algorithm. This is typically used to find the initial state for MCMC chains but can stand on its own as a clustering method. 

Agglomerative merging can be acomplished with the SBM class using the functions `collapse_blocks()` and `collapse_runs()`. `collapse_blocks()` runs a single agglomerative merge step from one group per node all the way down to your desired number of groups and `collapse_runs()` repeats this process for a range of groups.  

Before we look at using both we need to setup some data to cluster. 

## Setup

First we load the `sbmr` package along with the `tidyverse` package. 

```{r, eval = FALSE}
library(sbmr)
library(dplyr)
library(tidyr)
library(purrr)
library(ggplot2)
```

### Data

The data we are using will come from the included simulation function, `sim_basic_block_network()`.

```{r simulate_data}
set.seed(42) # seed for reproducability

n_blocks <- 3    # Four total groups
group_size <- 40 # W/ 50 nodes in each

network <- sim_basic_block_network(
  n_blocks = n_blocks,     
  n_nodes_per_block = group_size,  
  return_edge_propensities = TRUE,
  random_seed = 42 # This is the internal model seed (not same as set.seed())
)
```

We can investigate the simulated network's true block structure by visualizing the edge propensities between the groups.

```{r visualize_propensities}
network$edge_propensities %>% 
  ggplot(aes(x = block_1, y = block_2)) +
  geom_tile(aes(fill = propensity))
```

We can also visualise the simulated data directly using the `visualize_network()` function.

```{r visualize_network, eval = FALSE}
visualize_network(network)
```


## Single merge run

First we will demonstrate agglomerative merging using the single merge step function: `collapse_blocks()`. 

There are a few important input parameters we need to consider: 

- `sigma`: Controls the rate of collapse. At each step of the collapsing the model will try and remove `current_num_nodes(1 - 1/sigma)` nodes from the model. So a larger sigma means a faster collapse rate. 
- `desired_n_blocks`: How many groups should this given merge drop down to. If the network has more than one node type this number is multiplied by the total number of types. 
- `report_all_steps`: Should the model state be provided for every merge step or just the final one? If collapsing is being used to infer hierarcichal structure in data or inspection is desired this should be set to `TRUE`, otherwise it will slow down collapsing due to increased data transfer. 
- `greedy`: Should every possible group merger be considered? If `FALSE`, candidates for mergers are drawn by similarity in edges (just as MCMC move proposals are). This may lead the model to local minimums by always pursuing best possible merges.
- `num_block_proposals`: If `greedy = FALSE`, this parameter controls how many merger proposals are drawn for each group in the model. A larger number will increase the exploration of merge potentials but may lead the model to local minimums for the same reason greedy mode does. 
- `num_mcmc_sweeps`: How many MCMC sweeps the model does at each agglomerative merge step. This allows the model to allow nodes to find their most natural resting place in a given collapsed state. Larger values will slow down runtime but can potentially lead for more stable results. 

### Performing a single merge with 1 group per merge

To perform the complete merge we put our `sigma < 1` and our `num_mcmc_sweeps = 0`. In addition we return all the steps so we can look at the progress of the model as it collapses. 


```{r default_collapse_blocks}
network <- network %>% 
  collapse_blocks(sigma = 0.9, report_all_steps = TRUE)

network %>% 
  get_collapse_results() %>% 
  head()
```

The results of the collapse function are a column with entropy of the model at a given state, entropy delta caused by that move, and the number of groups in the model at that state. We can plot these using the included `visualize_collapse_results()` function to check for patterns. 

```{r plot_default_collapse}
visualize_collapse_results(network)
```

Right away we see a sharp leveling off of the entropy/ entropy delta after we get to the true number of groups. This shows that the model effectively found the true partitioning and then as soon as it was forced to merge non-member nodes together the performance got much worse.

### Deciding the end partition

While this example may seem simple enough that we could just select the optimal clusters by looking at the graph, we want an automated way to choose the best state that will get us away from guessing. There are a few included heuristics in the package (and you can build your own).

These heuristics try to guess the point where the model found the best partitioning by looking at the trends in the entropy delta value. The one that seems to work best is called "delta_ratio". it looks at the ratio of the average entropy delta pre and post a given location to find the point where there is the largest divergence, which usually indicates when the model has stopped being able to fill true groups into unique inferred groups. 

Any of these heuristics can be run on the entropy value instead of the entropy_delta just by setting the argument `use_entropy_value_for_score = TRUE`. 


#### Visualizing Collapse Results Scores

We can check out what a given heuristic looks like on our data by simply telling the `visualize_collapse_results()` function which one we want. 

```{r delta-heuristic-score}
axis_limits <- xlim(0,20)
true_num_clusters_line <- geom_vline(xintercept = n_blocks, color = 'orangered')

visualize_collapse_results(network, heuristic = "delta_ratio") +
  axis_limits +
  true_num_clusters_line +
  ggtitle("Delta ratio heuristic score")
```


So we see that the true number of clusters corresponds to the highest heuristic score, we can use this to automatically select our best grouping. First let's look at the other heuristic options.

__Non-linear least-squares residual__

This heueristic fits a non-linear regression model to the entropy as predicted by the number of groups. The equation it attempts to fit is $\text{entropy} \approx \beta_0 + \beta_1 \log(\text{n_blocks})$. This is based on the expected decay of entropy for a model with no true structure. Once this equation is fit each points residual from the fit is recorded. The point that is the farthest below the expected value according to the model is considered the 'best' fit. This is fit with `heuristic = 'nls_residual'`. 
 

```{r nls-score}
visualize_collapse_results(network, heuristic = "nls_residual") +
  axis_limits +
  true_num_clusters_line +
  ggtitle("NLS residual heuristic score")
```

__Deviation from rolling mean__

```{r dev-from-rolling-score}
visualize_collapse_results(network, heuristic = "dev_from_rolling_mean") +
  axis_limits +
  true_num_clusters_line +
  ggtitle("Deviation from rolling mean")
```

The deviation from the rolling mean function works similar to the NLS fit but uses a non-parametric smoother in a windowed mean that averages the last three values. 

#### Choosing

The function `choose_best_collapse_state()` will take a collapse results object and your model and use your heuristic of choice to find the best partitioning and then set the model to that. The same heuristics work for the `visualize_collapse_results()` and `choose_best_collapse_state()` functions. Lets choose our results using the deviation from rolling mean hueristic...

```{r choose-best-result-1}
network <- network %>% 
  choose_best_collapse_state(heuristic = "delta_ratio", verbose = TRUE)
```

So, as expected, the chosen best fit is the correct `n_blocks` blocks. 

### Investigating Results

Since we have the true groups for our nodes we can check to see how well the merging did. To do that we just need to merge our results and the raw network data and look at the contingency table of truth to inferred groups. 

```{r state-to-truth-1}
state_to_truth_table <- function(net){
  right_join(state(net),
             net$nodes,
             by = 'id') %>% 
    filter(level == 0) %>%
    rename(inferred = parent) %>% {
      table(.$inferred, .$block)
    }
}

state_to_truth_table(network)
```



So we can see that the merging algorithm perfectly separated our nodes into their true groups. 


### Allowing for node shuffling

Sometimes if we were worried that the model may be making bad merges early on that manifest in poor performance for smaller groups we may want to let the collapsing allow the nodes to find their preferred locations within each collapse. To do this we simply pause the collapsing and run a few mcmc sweeps to let the nodes find their natural resting place. This can help if a node or two accidentally got merged into an incorrect group earlier in the collapse. 

To enable this shuffing we simply set the argument `num_mcmc_sweeps` to a value greater than `0`. 

This will cause the algorithm to slow down a good bit because every each MCMC sweep move proposal takes O(N) (N = num nodes) time to complete. 

Let's try our above merging run with 5 MCMC sweeps after each step. 


```{r collapse_w_shuffle}
network <- network %>% 
  collapse_blocks(sigma = 0.9, 
                  num_mcmc_sweeps = 3,
                  report_all_steps = TRUE)

visualize_collapse_results(network, heuristic = 'dev_from_rolling_mean') +
  axis_limits +
  true_num_clusters_line +
  ggtitle("num_mcmc_sweeps = 10")
```

Now that we have allowed node shuffling the nice monotonicly decreasing entropy as the number of groups increases has gone away because our mcmc sweeps have the potential to improve the model fit more than the previous merge harmed it. We still however see a strong indicator of where our true number of blocks is. Let's select that value and see how the model performed. 

```{r state-to-truth-shuffled}
network %>% 
  choose_best_collapse_state(heuristic = "dev_from_rolling_mean", verbose = TRUE) %>% 
  state_to_truth_table()
```

After adding the sweeps we again get a perfect collapsing into the `n_block` generating blocks. 



## Raising sigma

One problem with these runs that collapse one group at a time is they are slow, when we add in MCMC sweeps they can get even slower. One way of dealing with this is the parameter `sigma`. 

`sigma` controls the rate the model is collapsed. A slower (and lower) value will allow the model to explore more intermediate steps and make less drastic jumps between. 

We can see how `sigma` effects the speed of the collapse by looking at the total number of steps needed to fully collapse a network down to one remaining group by the total number of nodes in the network. 


```{r steps_needed_to_collapse, echo = FALSE}
expand_grid(
  num_nodes = seq(10, 250, by = 10),
  sigma = exp(seq(log(1.1), log(2.5), length.out = 6))
) %>% 
  mutate(
    steps_to_collapse = map2_dbl(
      num_nodes, 
      sigma, 
      function(n_blocks, sigma){
        n_steps <- 0
        while(n_blocks > 1){
          merges_to_make <- max(1, round(n_blocks*(1 - (1/sigma))))
          n_blocks <- n_blocks - merges_to_make
          n_steps <- n_steps + 1
        }
        n_steps
      }
    )
  ) %>% 
  ggplot(aes(x = num_nodes, y = steps_to_collapse)) +
  geom_line() +
  facet_wrap(~round(sigma,3)) +
  labs(title = "Number of steps needed to fully collapse groups for different sigma values",
       x = "Number of starting nodes",
       y = "Steps needed to collapse")
```

We can see that at after around `sigma = 1.3` the algorithm takes very few steps to collapse the network even as the network gets large. 

If we pin the network size at a constant (here 150) we can see the path of collapsing in terms of groups remaining for a variety of sigmas...

```{r merge_traces}
exp(seq(log(1.1), log(2.5), length.out = 6)) %>% 
  map_dfr(function(sigma, starting_num = 150, num_steps = 15){
    steps <- 1:(num_steps-1) %>% 
      reduce(function(group_nums, i){
        curr_n_blocks <- group_nums[i]
        merges_to_make <- max(1, round(curr_n_blocks*(1 - (1/sigma))))
        c(group_nums, max(curr_n_blocks - merges_to_make, 1))
      },
      .init = c(starting_num))
    
    tibble(
      size = steps, 
      sigma = round(sigma, 3), 
      step = 1:num_steps
    )
  }) %>% 
  ggplot(aes(x = step, y = size)) +
  geom_line() +
  facet_wrap(~sigma) +
  labs(title = "Number of clusters remaining by merge step for different sigma values",
       y = "Number of clusters remaining",
       x = "Step Number")
```

Again we see that the model collapses very quickly at values of `sigma > 1.5`. 

We can see that with larger sigma values we get very rapid convergence to our desired model size. In theory this could cause issues like follows:

There are four groups in model (`{a,b,c,d}`). Groups `a` and `b` may be close to eachother and group `c` may be closer to `b` than it is do `d`. If we force two merges in a given step, `a`, and `b`, may be merged together and then, because those groups are merged and `c` is now left to merge with only `d`, which it does, forgoing the more optimal merging of `a->b->c`, `d`. In practive however, this scenario tends to not be an issue. 


Let's try setting sigma to `1.5` and seeing how both the collapse performs and how long it takes compared to an exhaustive collapse. 


First time the normal exhaustive collapse...
```{r timed-full-collapse}
full_collapse_time <- system.time({
  collapse_blocks(network, sigma = 0.9, report_all_steps = TRUE)
})

full_collapse_time
```


Now raise sigma to 1.4 and time...

```{r timed-high-sigma-collapse}
raised_sigma_collapse_time <- system.time({
  collapse_blocks(network, sigma = 1.4, report_all_steps = TRUE)
})

raised_sigma_collapse_time
```

Just by increasing sigma to 1.4 we have sped up the collapsing by `r round(full_collapse_time[1]/raised_sigma_collapse_time[1], 2)` times.

Speed isn't important if the model isnt working properly, however. So let's see how the collapse worked.

```{r visualize-high-sigma-collapse}
network <- collapse_blocks(network, sigma = 1.4, report_all_steps = TRUE)

visualize_collapse_results(network, heuristic = "dev_from_rolling_mean") +
  axis_limits +
  true_num_clusters_line +
  ggtitle("Collapse with sigma = 1.5")
```

So even with this speedup we are finding the proper number of clusters. Let's see if the separation is good. 

```{r high-sigma-state-to-truth}
network %>% 
  choose_best_collapse_state(heuristic = "dev_from_rolling_mean", 
                             verbose = TRUE) %>% 
  state_to_truth_table()
```

So again we are getting extremely good performance. Often a balance between allowing MCMC sweeps and a moderate sigma value will return very good and very fast collapsing results. 


## Collapse Runs

So far we have looked at a single run of agglomerative collapsing, but the recomended method in the paper the algorithm was introduced in, [Piexoto 2014](https://arxiv.org/abs/1310.4378) is running a seperate collapse for each reasonable number of groups, going from one up to a reasonable stopping point. 

Interestingly, due to the speed advantages of the high sigma values, even running seperate collapses for each target group number can be very fast compared to a single merge that looks at the possible group values. 

The function `collapse_run()` performs multiple merge runs to different numbers of groups and returns the results of the final merge collapsed together in the same form `collapse_blocks()` does with `report_all_steps = TRUE`. 

The only arguments it has that `collapse_blocks()` does not are:

- `num_final_blocks`: Array of group numbers that the agglomerative merge should target. E.g. 1:10.
- `parallel`: Should algorithm be run in parallel to speed up?

The parallel argument is particularly valuable as it exploits the fact that each run is independent of the others and thus can be run on seperate threads for significant speedups. 

Lets try it on our model scanning from 1-10 groups using sigma of `1.5` and `5` mcmc sweeps for each individual collapse. 

```{r parallel_collapse_run}
network <- network %>% 
  collapse_run(num_final_blocks = 1:10, 
               sigma = 1.4,
               num_mcmc_sweeps = 5,
               parallel = TRUE)
```



We can use the same collapse viewing and selection functions as we did previously. 

```{r visualize-collapse-run}
network %>% 
  visualize_collapse_results(heuristic = "nls_residual") +
  true_num_clusters_line +
  ggtitle("Collapse run")
```


```{r collapse-run-state-to-truth}
network %>% 
  choose_best_collapse_state(heuristic = "nls_residual", 
                             verbose = TRUE, 
                             use_entropy = TRUE) %>% 
  state_to_truth_table()
```


So we can get MCMC sweeps and moderate sigma values and maintain rather fast collapsing. The nice thing is every end point is targeted directly so the model has the best chance to find the optimal partition for that given number of blocks. 


## Effects of other parameters on merge paths

While sigma is the most important parameter for collapse results tuning there are others we can tune. Let's look at how they effect the results. 

## Epsilon values

Epsilon controls how conservative the model is when generating those proposals. As epsilon goes to infinity the moves proposed by the model approach randomness. When epsilon is zero a node can only move to groups that nodes it is connected belong to.


```{r toggling_epsilon}
# Get logaritmically spaced epsilon range
eps_vals <- exp(seq(log(0.1), log(15), length.out = 8))

eps_sweep_results <- purrr::map_dfr(
  eps_vals, 
  function(eps){
    collapse_run(
      network,
      num_final_blocks = 1:8,
      eps = eps,
      parallel = TRUE
    ) %>% 
      get_collapse_results() %>% 
      mutate(epsilon = eps)
  }
)

eps_sweep_results %>% 
  ggplot(aes(x = n_blocks, y = entropy, color = epsilon)) +
  geom_line(aes(group = epsilon)) +
  geom_point() +
  scale_y_log10() +
  ggtitle("Collapse results by epsilon value")
```

While the results are close to eachother, there seems to be a trend of higher epsilon values getting better results.

## Looking at stability of collapse runs

By simply passing in a groups numbers to check vector with repeates in it we can acccess how stable a given collapse value is for a network. Here we will do five repeats for our network for the range of 2-8 groups. 

```{r run_stability}
num_repeats <- 5
group_nums <- 1:8

network <- network %>% 
  collapse_run(
    num_final_blocks = rep(group_nums, each = num_repeats),
    num_mcmc_sweeps = 5,
    parallel = TRUE
  )

network %>% 
  get_collapse_results() %>% 
  ggplot(aes(x = n_blocks, y = entropy)) +
  geom_smooth(size = 0, method = 'loess', span = 0.75, fill = 'steelblue') +
  geom_jitter(alpha = 0.5, height = 0, width = 0.2)  +
  scale_x_continuous(breaks = group_nums)
```

We can see that the results are very stable with relatively minescule for values greater than 3 groups and no variation in the others (indicating they acheived the same partitioning every time). 


